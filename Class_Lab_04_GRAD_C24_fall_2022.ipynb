{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz8FQLlCTwSZBgAXU3k1Bn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hertie-School-Machine-Learning-F2022/Class_lab_04/blob/main/Class_Lab_04_GRAD_C24_fall_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proprocessing\n",
        "\n",
        "Types of data:\n",
        "Numerical | categorical (Do we have more names for these two?)\n",
        "\n",
        "What about ordinal? \n",
        "Examples of ordinal? \n",
        "\n",
        "We will first take a look at preprocessing data for numerical data. "
      ],
      "metadata": {
        "id": "x-ubxCaHH6lI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import some libraries that we will use\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt"
      ],
      "metadata": {
        "id": "e3eapRXPfTNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are multiple ways to load data into a google colab notebook, let's take a look at one that takes a file from your local machine, and allows you to run it in the cloud."
      ],
      "metadata": {
        "id": "Zkn25Wbefgd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First upload the file to the session\n",
        "\n"
      ],
      "metadata": {
        "id": "rGVGOmHFff-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then read the file as a csv\n",
        "\n"
      ],
      "metadata": {
        "id": "IBTV2FNNf4mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a look at the data"
      ],
      "metadata": {
        "id": "n9Lgf5EpHvQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can take a look at the first couple of samples\n"
      ],
      "metadata": {
        "id": "5TQUcKUPfdAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also get a statistical description of the data\n"
      ],
      "metadata": {
        "id": "jQedQ6s8Hzmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How can we set X and y values with our own data?\n",
        "\n"
      ],
      "metadata": {
        "id": "N2OO4QBegwEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at X\n"
      ],
      "metadata": {
        "id": "CP5WzFngtbWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does the data look after seeing the description?\n",
        "\n"
      ],
      "metadata": {
        "id": "dPhd7393g7aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First type of preprocessing that we have already seen: Standard Scaler\n",
        "\n",
        "z = (x - u) / s\n",
        "\n",
        "where u is the mean of the training samples , and s is the standard deviation of the training samples.\n"
      ],
      "metadata": {
        "id": "B3FXYtb3I2-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Scaler\n"
      ],
      "metadata": {
        "id": "KpAXLWB7hAa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at another type of 'scaler' the Quantile transformer\n",
        "\n",
        "This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme."
      ],
      "metadata": {
        "id": "0sv41n9nJVKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QuantileTransformer\n"
      ],
      "metadata": {
        "id": "W-6cdNYHhmxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What other preprocesing strategies do we know? \n",
        "\n",
        "Interactions and polynomials\n",
        "\n",
        "What does an interaction do? \n",
        "\n",
        "How does a polynomial behave? X^2, X^3...?"
      ],
      "metadata": {
        "id": "ljjY-9SJJdfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at more complex data"
      ],
      "metadata": {
        "id": "Vvh80RBoinWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import new file preproc2.csv\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['preproc2.csv']))"
      ],
      "metadata": {
        "id": "MDr2LUO2ipWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does it look like \n",
        "\n"
      ],
      "metadata": {
        "id": "bohDShDji4rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that this data can not be separeted into two simple classes\n",
        "# with a line like logistic regression would do\n"
      ],
      "metadata": {
        "id": "ycCHTXgrmKic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonlinear features\n",
        "\n",
        "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. \n",
        "\n",
        "For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]."
      ],
      "metadata": {
        "id": "f1-xmXGqLT50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Polynomial Features\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "#\n",
        "\n",
        "X_new.rename(columns={0:\"1\",\n",
        "                      1:\"a\",\n",
        "                      2:\"b\",\n",
        "                      3:\"a^2\",\n",
        "                      4:\"ab\",\n",
        "                      5:\"b^2\"\n",
        "                      } ,inplace=True)\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "iFQiTwITjM78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets implement the prediction pipeline again and see how \n",
        "# it behaves \n",
        "\n"
      ],
      "metadata": {
        "id": "c7jdzHhfl6Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For categorical data\n",
        "\n",
        "what is categorical data?\n",
        "\n",
        "How can we preprocess categorical data?"
      ],
      "metadata": {
        "id": "zkm6iaYymUHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will create a fake array to take a look at one hot encoder or 'dummy encoding' \n",
        "\n",
        "arr = np.array([\"always\", \"almost always\", \"almost never\", \"never\"]).reshape(-1, 1)\n",
        "arr"
      ],
      "metadata": {
        "id": "_iFBOeVImWs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding - dummy encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "nBzOv97Gn2Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What happens if we add a new category that it has never seen?\n"
      ],
      "metadata": {
        "id": "ADR893ssn6im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to set the parameter 'handle unknown' to 'ignore'\n"
      ],
      "metadata": {
        "id": "IEc6rLTvou1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all this information, we are ready for a full pipeline with both\n",
        "categorical and numerical data"
      ],
      "metadata": {
        "id": "fpqFpERoMa_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a fake dataset with mixed data types\n",
        "\n",
        "parents = [0,1,2,2,1]\n",
        "years = [5,5,4,3,4]\n",
        "sex = ['M', 'F', 'F', 'M', 'M']\n",
        "rent = ['R', 'O', 'R', 'R', 'O']\n",
        "\n",
        "df = {'parents':parents,\n",
        "      'years':years,\n",
        "      'sex':sex,\n",
        "      'rent':rent}\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "df"
      ],
      "metadata": {
        "id": "7sp6c2chp1XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First transform the sex variable\n"
      ],
      "metadata": {
        "id": "0_mP4NcMpXAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What categories are there? \n",
        "\n"
      ],
      "metadata": {
        "id": "VpPX2LlNpyjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the pipeline to preprocess both categorical and numerical data\n",
        "# we need a column transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "Vqoizk-dq-br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will fit a full pipeline with a real data set that has both numerical and categorical data of abalones (shellfish) and try to predict the age of the ablone (less than 11 years old or more than 11 years old)"
      ],
      "metadata": {
        "id": "VMiucQrHOBwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "kWKahkAS2PHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['abalone.csv']))"
      ],
      "metadata": {
        "id": "AAgEN7c93KzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data inspection \n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "ASWU2tvEAR77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will drop Rings, Age (Rings + 1.5), and Older because it is what we are trying\n",
        "# to classify, and set X\n",
        "\n",
        "\n",
        "\n",
        "# We will set y to 'Older'\n"
      ],
      "metadata": {
        "id": "8LmoDYiX4ksd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What X looks like now\n",
        "\n"
      ],
      "metadata": {
        "id": "NPEMt82kBDMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will do a train test split to begin out pipeline\n"
      ],
      "metadata": {
        "id": "fl7YPP9KDIhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets set a column_trans step to make sure the model can handel all of our\n",
        "# data in the process\n"
      ],
      "metadata": {
        "id": "UqZj6Gv5BMSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now implement the whole pipeline to classify abalones \n",
        "\n",
        "\n",
        "# Use a logistic regression model \n",
        "\n",
        "\n",
        "# Prepare our piple with 1.- Preprocessing step and 2.- Logistic Regression\n",
        "\n",
        "\n",
        "# We will do cross validation on the train set, 3 times\n"
      ],
      "metadata": {
        "id": "_c5ca5dAsL3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}